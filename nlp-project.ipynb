{"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n\n# **Text Classification for Spam**\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"center\">\n <img src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image4_v2LFcq0.max-1200x1200.png\" alt=\"drawing\" width=\"1000\"/>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"# **1. Business Objective**\n\n## **Identify the business task and translate it into an NLP Task**\n\nThis project aims to develop a robust system for identifying spam emails, translating the business need into an NLP task. The goal is to differentiate between legitimate messages and spam, thereby reducing time wasted on malicious emails.\n\n## **Consider key stakeholders**\n\n- **IT Department**: Responsible for maintaining and improving email filtering systems.\n- **Employees**: Will benefit from a cleaner inbox and reduced time spent dealing with spam.\n- **Company Management**: Interested in improving overall productivity and reducing security risks associated with spam emails (e.g., phishing scams).\n\n## **Objectives**\n\n- **Obtain the dataset**: Gather a suitable dataset of email messages labeled as spam or legitimate.\n- **Set up the environment and download all dependencies**: Ensure all necessary software and libraries are installed.\n- **Pre-process the data**: Clean and preprocess the email data, including tasks such as removing HTML tags, stopwords, and punctuation.\n- **Perform feature engineering**: Extract relevant features from the text data, such as word frequency, n-grams, and TF-IDF scores.\n- **Model selection**: Choose appropriate machine learning or deep learning models for text classification, considering factors such as performance and scalability.\n- **Training and testing the model**: Train the selected model on the preprocessed data and evaluate its performance using appropriate metrics.\n- **Implement the final model and test on emails**: Deploy the trained model to classify incoming emails as spam or legitimate and assess its effectiveness in a real-world scenario.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **2. Setting up the Environment**\n\n## **Key Objectives**\n\n- **Import Required Libraries**: Load essential libraries for data manipulation (`pandas`, `numpy`), preprocessing (`StandardScaler`, `MinMaxScaler`), feature engineering (`SelectKBest`, `PCA`), handling imbalanced data (`SMOTE`, `RandomUnderSampler`), model building (`LogisticRegression`, `RandomForestClassifier`), and evaluation (`accuracy_score`).\n- **Visualization Tools**: Import visualization libraries (`seaborn`, `matplotlib`) for data analysis and presentation.\n- **Install Necessary Packages**: Ensure external libraries are installed (`ucimlrepo`).\n- **Environment Configuration**: Set up any required environment variables or settings.\n\nThis section ensures all necessary libraries and tools are available for data analysis, model building, and evaluation.","metadata":{}},{"cell_type":"code","source":"# Setting the environment by importing all the required modules and libraries\nimport os\nimport requests\nimport re\n\n# Data manipulation and preprocessing libraries and modules\nimport pandas as pd\nimport numpy as np\n\n# Data pre-processing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Feature engineering\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\nfrom sklearn.decomposition import PCA\n\n# Handling imbalanced data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Install additional libraries\n!pip install ucimlrepo","metadata":{"id":"1pzP3ofW7S8D","execution":{"iopub.status.busy":"2024-06-05T17:51:41.729285Z","iopub.execute_input":"2024-06-05T17:51:41.729689Z","iopub.status.idle":"2024-06-05T17:52:02.115245Z","shell.execute_reply.started":"2024-06-05T17:51:41.729643Z","shell.execute_reply":"2024-06-05T17:52:02.113699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. System for Fetching Latest Dataset**\n\n## **Key Objectives**\n\n- **Reflecting Current Trends and Patterns**: Ensure the dataset reflects the most recent trends by fetching the latest version from the source repository.\n- **Improved Accuracy and Performance**: Regularly update the dataset to improve model accuracy and performance by using the most up-to-date data.\n- **Continuous Improvement**: Automate the data retrieval process to ensure continuous improvement and seamless updates.\n\nThis section describes the automated system for fetching and loading the latest version of the dataset to keep the analysis current and relevant.\nIt workes hand in hand with with the GitHub Actions workflows that has been set up to run every sunday midnight to retrieve the latest dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport requests\nimport re\nfrom ucimlrepo import fetch_ucirepo\n\ndef fetch_current_version():\n    \"\"\"Fetch the current dataset version from the GitHub repository.\"\"\"\n    url = \"https://raw.githubusercontent.com/uci-ml-repo/ucimlrepo/main/setup.py\"\n    response = requests.get(url)\n    \n    if response.status_code == 200:\n        version_match = re.search(r\"version='(.*?)'\", response.text)\n        if version_match:\n            return version_match.group(1)\n    return None\n\ndef fetch_and_load_dataset():\n    \"\"\"Fetch and load the dataset.\"\"\"\n    spambase = fetch_ucirepo(id=94)\n    \n    # Convert data to DataFrames\n    data_df = pd.DataFrame(spambase.data.features)\n    target_df = pd.DataFrame(spambase.data.targets, columns=[\"Class\"])\n    \n    # Combine features and target variable into a single DataFrame\n    df = pd.concat([data_df, target_df], axis=1)\n    \n    return df\n\ndef main():\n    \"\"\"Main function to automate the data retrieval process and use the DataFrame directly.\"\"\"\n    current_version = fetch_current_version()\n    if not current_version:\n        print(\"Failed to fetch the current version from the repository.\")\n        return None\n    \n    print(f\"Current dataset version: {current_version}\")\n    \n    # Fetch and load the dataset\n    df = fetch_and_load_dataset()\n    \n    # Display the first few rows of the dataset\n    print(df.head())\n    \n    return df\n\n# Execute the main function and load the DataFrame\ndf = main()\n\n# Continue with your data processing and model training\nif df is not None:\n    # Example: Show the first few rows of the DataFrame\n    print(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:02.118153Z","iopub.execute_input":"2024-06-05T17:52:02.118950Z","iopub.status.idle":"2024-06-05T17:52:03.250891Z","shell.execute_reply.started":"2024-06-05T17:52:02.118909Z","shell.execute_reply":"2024-06-05T17:52:03.249812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Data Exploration**\nReflecting current trends and patterns\n\nThe data exploration phase involves understanding the underlying trends and patterns in the dataset to gain insights that will guide further analysis and model development.\n\n## Key Objectives:\n* **Understand the Data Structure:** Load and inspect the dataset to get an overview of the data.\n* **Data Cleaning:** Identify and handle missing values, and remove duplicates to ensure data quality.\n* **Feature Understanding and Analysis:** Analyze summary statistics and distributions of numerical and categorical features.\n* **Outlier Detection:** Detect and address outliers using statistical methods and visualization techniques.\n* **Correlation Analysis:** Compute and visualize correlations between numerical features to identify relationships.\n* **Trend and Pattern Analysis:** For time-related data, identify trends and seasonal patterns; analyze data grouped by categorical features.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:03.252418Z","iopub.execute_input":"2024-06-05T17:52:03.252845Z","iopub.status.idle":"2024-06-05T17:52:03.291837Z","shell.execute_reply.started":"2024-06-05T17:52:03.252804Z","shell.execute_reply":"2024-06-05T17:52:03.290718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is high-dimensional,having a total of 57 features and the target variable: \"Class\". Each feature describes the the frequency of words or characters in the text.\nHaving a dataset that is high-dimensional poses several challenges such as computational complexity, and overfitting\nAs a result, I will have to address this issue later in the analysis using methods such as dimensional reduction or feture selection","metadata":{}},{"cell_type":"code","source":"data_type = df.dtypes\ndata_types_counts = data_type.value_counts()\n\nprint('Number of columns with each data type: ')\nprint(data_types_counts)","metadata":{"id":"-cM1yaNiBhjb","outputId":"e564afa6-28b3-49e2-e21f-fdbd55861ee3","execution":{"iopub.status.busy":"2024-06-05T17:52:03.293321Z","iopub.execute_input":"2024-06-05T17:52:03.293754Z","iopub.status.idle":"2024-06-05T17:52:03.307491Z","shell.execute_reply.started":"2024-06-05T17:52:03.293715Z","shell.execute_reply":"2024-06-05T17:52:03.306207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary statistics\ndf.describe()","metadata":{"id":"6e1RhF5YNUBn","outputId":"543260c5-97ea-4dc8-d179-987a2d39418d","execution":{"iopub.status.busy":"2024-06-05T17:52:03.310653Z","iopub.execute_input":"2024-06-05T17:52:03.311013Z","iopub.status.idle":"2024-06-05T17:52:03.463403Z","shell.execute_reply.started":"2024-06-05T17:52:03.310983Z","shell.execute_reply":"2024-06-05T17:52:03.462248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate skewness and kurtosis for all numerical columns\nskewness = df.skew()\nkurtosis = df.kurtosis()\n\n# Create DataFrames for skewness and kurtosis\nskew_df = pd.DataFrame(skewness, columns=['Skewness'])\nkurtosis_df = pd.DataFrame(kurtosis, columns=['Kurtosis'])\n\n# Concatenate skewness and kurtosis DataFrames\nsummary_df = pd.concat([skew_df, kurtosis_df], axis=1)\n\n# Display the summary DataFrame\nprint(summary_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:03.464856Z","iopub.execute_input":"2024-06-05T17:52:03.465246Z","iopub.status.idle":"2024-06-05T17:52:03.496152Z","shell.execute_reply.started":"2024-06-05T17:52:03.465216Z","shell.execute_reply":"2024-06-05T17:52:03.494875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset exhibits notable skewness and kurtosis in several features, indicating non-normal distributions and potential outliers. Features such as 'word_freq_3d', 'word_freq_parts', and 'char_freq_#' display particularly high positive skewness and kurtosis, suggesting heavy tails and potential outliers. Further exploration and preprocessing may be needed to address these characteristics effectively","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:04:04.790250Z","iopub.execute_input":"2024-04-25T18:04:04.790721Z","iopub.status.idle":"2024-04-25T18:04:04.803476Z","shell.execute_reply.started":"2024-04-25T18:04:04.790672Z","shell.execute_reply":"2024-04-25T18:04:04.801637Z"}}},{"cell_type":"code","source":"# Identify columns with missing values\ndf.isnull().sum()","metadata":{"id":"A0zgY7BK7Fs6","outputId":"7b8bdc6b-99b8-41da-b696-b1e3bd286255","execution":{"iopub.status.busy":"2024-06-05T17:52:03.497532Z","iopub.execute_input":"2024-06-05T17:52:03.497957Z","iopub.status.idle":"2024-06-05T17:52:03.509733Z","shell.execute_reply.started":"2024-06-05T17:52:03.497917Z","shell.execute_reply":"2024-06-05T17:52:03.508570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"id":"22UJqGMR7lEz","execution":{"iopub.status.busy":"2024-06-05T17:52:03.511658Z","iopub.execute_input":"2024-06-05T17:52:03.512097Z","iopub.status.idle":"2024-06-05T17:52:03.537876Z","shell.execute_reply.started":"2024-06-05T17:52:03.512041Z","shell.execute_reply":"2024-06-05T17:52:03.536538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = df['Class'].value_counts()\n\n# Colors for the pie chart\ncolors = ['#66c2a5', '#fc8d62']\n\n# Explode the second slice (1) to highlight it\nexplode = (0, 0.1)\n\n# Create a pie chart\nplt.figure(figsize=(5, 5))\nplt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140, colors=colors, explode=explode, shadow=True, wedgeprops={'linewidth': 3, 'edgecolor': 'white'})\nplt.title('Distribution of Non-Fradulent and Spam emails', fontsize=16, fontweight='bold', color='navy')\n\n# Add a legend with custom formatting\nplt.legend(title='Class', loc='upper left', labels=['Non-Fraudulent (0)', 'Fraudulent (1)'], fontsize=8, bbox_to_anchor=(0.9, 0.95))\n\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.axis('equal')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:03.539258Z","iopub.execute_input":"2024-06-05T17:52:03.539604Z","iopub.status.idle":"2024-06-05T17:52:03.841257Z","shell.execute_reply.started":"2024-06-05T17:52:03.539573Z","shell.execute_reply":"2024-06-05T17:52:03.840123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the correlation matrix\ncorrelation_matrix = df.corr()\n\n# Set up the matplotlib figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=0.5)\n\n# Add title\nplt.title('Correlation Heatmap of Features')\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:03.843029Z","iopub.execute_input":"2024-06-05T17:52:03.843723Z","iopub.status.idle":"2024-06-05T17:52:04.822822Z","shell.execute_reply.started":"2024-06-05T17:52:03.843682Z","shell.execute_reply":"2024-06-05T17:52:04.821535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Data Preprocessing**\n\n### **Key Tasks (Where Applicable)**\n\n- **Handling Missing Values**: Identify and fill or remove missing data to ensure the dataset is complete.\n- **Addressing Non-Normal Distributions**: Transform or scale features to achieve normal distribution if necessary.\n- **Removing Duplicate Values**: Detect and eliminate duplicate entries to maintain data integrity.\n- **Normalization**: Scale features to a standard range to ensure consistent data representation.\n- **Managing Imbalanced Data**: Use techniques such as oversampling, undersampling, or synthetic data generation to balance the classes.","metadata":{"execution":{"iopub.status.busy":"2024-04-09T23:22:06.753480Z","iopub.execute_input":"2024-04-09T23:22:06.753973Z","iopub.status.idle":"2024-04-09T23:22:06.761861Z","shell.execute_reply.started":"2024-04-09T23:22:06.753938Z","shell.execute_reply":"2024-04-09T23:22:06.760404Z"}}},{"cell_type":"markdown","source":"## 5a. Handling missing values.","metadata":{}},{"cell_type":"code","source":"#### The function will return features that have missing values.(If any)\n\ndef features_with_missing_values(df):\n    \"\"\"\n    Returns:\n        > list of features that contains missing values\n    \n    Parameter:\n        > df\n    \n    Returns:\n        > list of features\n    \n    \"\"\"\n    return df.columns[df.isnull().any()].tolist()\n\nmissingFeatures = features_with_missing_values(df)\nprint(f\"Features with missing values: {missingFeatures}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:04.824197Z","iopub.execute_input":"2024-06-05T17:52:04.824556Z","iopub.status.idle":"2024-06-05T17:52:04.833769Z","shell.execute_reply.started":"2024-06-05T17:52:04.824524Z","shell.execute_reply":"2024-06-05T17:52:04.832502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The list is empty meaning, there are no missing values in the dataset","metadata":{}},{"cell_type":"markdown","source":"## 5b. Duplicate values.","metadata":{}},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:04.835549Z","iopub.execute_input":"2024-06-05T17:52:04.835897Z","iopub.status.idle":"2024-06-05T17:52:04.896947Z","shell.execute_reply.started":"2024-06-05T17:52:04.835866Z","shell.execute_reply":"2024-06-05T17:52:04.895880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the DataFrame to show only duplicated rows\nduplicated_rows = df[df.duplicated()]\n\n# Count the occurrences of each class within the duplicated rows\nclass_distribution = duplicated_rows['Class'].value_counts()\n\n# Display the distribution\nprint(\"Distribution of Classes within Duplicated Rows:\")\nprint(class_distribution)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:04.898509Z","iopub.execute_input":"2024-06-05T17:52:04.898931Z","iopub.status.idle":"2024-06-05T17:52:04.921683Z","shell.execute_reply.started":"2024-06-05T17:52:04.898893Z","shell.execute_reply":"2024-06-05T17:52:04.920446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decided to drop the duplicates as they may cause the model to overfit to the training data, as it learns patterns that are specific to the duplicates rather than general spam characteristics. This can lead to poor generalization performance on unseen data.","metadata":{}},{"cell_type":"code","source":"df = df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:04.926422Z","iopub.execute_input":"2024-06-05T17:52:04.926782Z","iopub.status.idle":"2024-06-05T17:52:04.946862Z","shell.execute_reply.started":"2024-06-05T17:52:04.926754Z","shell.execute_reply":"2024-06-05T17:52:04.945392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5c. Handling Imbalanced dataset.\n#### Perfoming oversampling can achieve a balance between two classes","metadata":{}},{"cell_type":"code","source":"# Separate the features (X) and the target variable (y)\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Instantiate SMOTE with sampling_strategy='minority' to only oversample the minority class\nsmote = SMOTE(sampling_strategy='minority')\n\n# Apply SMOTE to resample the data\nX_resampled, y_resampled = smote.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:04.948581Z","iopub.execute_input":"2024-06-05T17:52:04.948947Z","iopub.status.idle":"2024-06-05T17:52:05.182339Z","shell.execute_reply.started":"2024-06-05T17:52:04.948915Z","shell.execute_reply":"2024-06-05T17:52:05.181396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_resampled.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:05.183551Z","iopub.execute_input":"2024-06-05T17:52:05.184604Z","iopub.status.idle":"2024-06-05T17:52:05.192830Z","shell.execute_reply.started":"2024-06-05T17:52:05.184558Z","shell.execute_reply":"2024-06-05T17:52:05.191588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creat a new dataframe\ndf_clean = pd.DataFrame(X_resampled, columns=X.columns)\ndf_clean['Target'] = y_resampled","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:05.194511Z","iopub.execute_input":"2024-06-05T17:52:05.194895Z","iopub.status.idle":"2024-06-05T17:52:05.204540Z","shell.execute_reply.started":"2024-06-05T17:52:05.194866Z","shell.execute_reply":"2024-06-05T17:52:05.203488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean['Target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:05.206322Z","iopub.execute_input":"2024-06-05T17:52:05.206763Z","iopub.status.idle":"2024-06-05T17:52:05.219226Z","shell.execute_reply.started":"2024-06-05T17:52:05.206725Z","shell.execute_reply":"2024-06-05T17:52:05.218085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = df_clean['Target'].value_counts()\n\n# Colors for the pie chart\ncolors = ['#66c2a5', '#fc8d62']\n\n# Explode the second slice (1) to highlight it\nexplode = (0, 0.1)\n\n# Create a pie chart\nplt.figure(figsize=(8, 8))\nplt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140, colors=colors, explode=explode, shadow=True, wedgeprops={'linewidth': 3, 'edgecolor': 'white'})\nplt.title('Distribution of Classes', fontsize=16, fontweight='bold', color='navy')\n\n# Add a legend with custom formatting\nplt.legend(title='Class', loc='upper left', labels=['Not Spam (0)', 'Spam (1)'], fontsize=12, bbox_to_anchor=(0.85, 0.95))\n\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.axis('equal')\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:05.220522Z","iopub.execute_input":"2024-06-05T17:52:05.221041Z","iopub.status.idle":"2024-06-05T17:52:05.491786Z","shell.execute_reply.started":"2024-06-05T17:52:05.221011Z","shell.execute_reply":"2024-06-05T17:52:05.490709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5c. Handling skeweness in the data.","metadata":{}},{"cell_type":"code","source":"df_clean.hist(figsize=(22,22))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:05.493131Z","iopub.execute_input":"2024-06-05T17:52:05.493453Z","iopub.status.idle":"2024-06-05T17:52:16.185813Z","shell.execute_reply.started":"2024-06-05T17:52:05.493425Z","shell.execute_reply":"2024-06-05T17:52:16.184611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen from the histograms, most of them are skewed to the left. Indicating that the distribution is highly concentrated on the lower values.","metadata":{}},{"cell_type":"code","source":"# Define threshold for considering features as highly skewed\nthreshold = 3\n\n# Select highly skewed features\nhighly_skewed_features = df_clean.columns[np.abs(df_clean.skew()) > threshold]\n\n# Apply appropriate transformations to highly skewed features\nfor feature in highly_skewed_features:\n    # Log transformation (adding a small constant to handle zero values)\n    df_clean[feature] = np.log1p(df_clean[feature])","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:16.187589Z","iopub.execute_input":"2024-06-05T17:52:16.188373Z","iopub.status.idle":"2024-06-05T17:52:16.224722Z","shell.execute_reply.started":"2024-06-05T17:52:16.188329Z","shell.execute_reply":"2024-06-05T17:52:16.223503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5d. Normalizing the dataset\n### Normalize the datatset to equalize their influence on the model","metadata":{}},{"cell_type":"code","source":"# Normalize the datatset to equalize their influence on the model\n\n# Extract features and excluding the target var\nX = df_clean.drop('Target',axis=1)\n\n# Extract the target variable (y)\ny = df_clean['Target']\n\nscaler = StandardScaler()\n\n# Normalize the features\nX_normalized = scaler.fit_transform(X)\n\n# Covert normalized array to df\ndf = pd.DataFrame(X_normalized,columns=X.columns)\ndf['Target'] = y","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:16.226035Z","iopub.execute_input":"2024-06-05T17:52:16.226408Z","iopub.status.idle":"2024-06-05T17:52:16.260464Z","shell.execute_reply.started":"2024-06-05T17:52:16.226377Z","shell.execute_reply":"2024-06-05T17:52:16.259410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. Feature Engineering**\n\n### Key tasks:\n* Create relevant features that capture key information","metadata":{}},{"cell_type":"markdown","source":"### Feature selection using: \n* ANOVA F-test\n* Recursive Feature Elimination (RFE)\n* L1-based regularization (Lasso)","metadata":{}},{"cell_type":"code","source":"X = df.drop('Target',axis=1)\n\ny= df['Target']","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:16.261731Z","iopub.execute_input":"2024-06-05T17:52:16.262047Z","iopub.status.idle":"2024-06-05T17:52:16.268909Z","shell.execute_reply.started":"2024-06-05T17:52:16.262021Z","shell.execute_reply":"2024-06-05T17:52:16.267740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6a. Anova F-test\nHigher F-test scores indicate greater importance, as these features are more likely to be informative for predicting the target variable","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:36:29.277078Z","iopub.execute_input":"2024-04-11T11:36:29.277549Z","iopub.status.idle":"2024-04-11T11:36:29.283765Z","shell.execute_reply.started":"2024-04-11T11:36:29.277504Z","shell.execute_reply":"2024-04-11T11:36:29.282311Z"}}},{"cell_type":"code","source":"def load_data():\n    # Load your dataset\n    df\n    return df\n\ndef select_features_anova(X, y, k=10):\n    k_best = SelectKBest(score_func=f_classif, k=k)\n    k_best.fit(X, y)\n    selected_indices = k_best.get_support(indices=True)\n    selected_features = X.columns[selected_indices]\n    plot_anova_scores(X, k_best.scores_)\n    return selected_features\n\ndef plot_anova_scores(X, scores):\n    plt.figure(figsize=(15, 6))\n    plt.bar(range(len(scores)), scores, tick_label=X.columns)\n    plt.xlabel('Features')\n    plt.ylabel('ANOVA F-test score')\n    plt.title('ANOVA F-test scores for feature selection')\n    plt.xticks(rotation=90)\n    plt.show()\n\ndef train_and_evaluate_model(X, y, selected_features):\n    X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=42)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\ndef main():\n    df = load_data()\n    X = df.drop('Target', axis=1)\n    y = df['Target']\n    \n    global selected_features_anova\n    selected_features_anova = select_features_anova(X, y)\n    train_and_evaluate_model(X, y, selected_features_anova)\n\nif __name__ == \"__main__\":\n    main()\n\n# Accessing selected_features_anova outside the main function\nprint(\"Selected features using ANOVA F-test:\", selected_features_anova)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:16.270731Z","iopub.execute_input":"2024-06-05T17:52:16.271239Z","iopub.status.idle":"2024-06-05T17:52:17.118725Z","shell.execute_reply.started":"2024-06-05T17:52:16.271199Z","shell.execute_reply":"2024-06-05T17:52:17.116983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6b. Recursive Feature Elimination (RFE)\nRFE is an iterative feature selection method that starts with all features and recursively removes the least important features based on the coefficients of a specified machine learning model\nLower ranking values indicate higher importance, as these features were selected earlier during the RFE process","metadata":{"execution":{"iopub.status.busy":"2024-04-11T11:36:48.200095Z","iopub.execute_input":"2024-04-11T11:36:48.201382Z","iopub.status.idle":"2024-04-11T11:36:48.206392Z","shell.execute_reply.started":"2024-04-11T11:36:48.201341Z","shell.execute_reply":"2024-04-11T11:36:48.205085Z"}}},{"cell_type":"code","source":"def load_data():\n    # Load your dataset\n    df\n    return df\n\ndef select_features_rfe(X, y, n_features=20):\n    model = LogisticRegression()\n    rfe = RFE(estimator=model, n_features_to_select=n_features)\n    rfe.fit(X, y)\n    selected_indices = rfe.get_support(indices=True)\n    selected_features = X.columns[selected_indices]\n    plot_rfe_rankings(X, rfe.ranking_)\n    return selected_features\n\ndef plot_rfe_rankings(X, rankings):\n    plt.figure(figsize=(15, 6))\n    plt.bar(range(len(rankings)), rankings, tick_label=X.columns)\n    plt.xlabel('Features')\n    plt.ylabel('Ranking')\n    plt.title('Feature Rankings')\n    plt.xticks(rotation=90)\n    plt.show()\n\ndef train_and_evaluate_model(X, y, selected_features):\n    X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=42)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\ndef main():\n    df = load_data()\n    X = df.drop('Target', axis=1)\n    y = df['Target']\n    \n    global selected_features_rfe\n    selected_features_rfe = select_features_rfe(X, y)\n    train_and_evaluate_model(X, y, selected_features_rfe)\n\nif __name__ == \"__main__\":\n    main()\n\n# Accessing selected_features_rfe outside the main function\nprint(\"Selected features using RFE:\", selected_features_rfe)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:17.121105Z","iopub.execute_input":"2024-06-05T17:52:17.121739Z","iopub.status.idle":"2024-06-05T17:52:20.273448Z","shell.execute_reply.started":"2024-06-05T17:52:17.121686Z","shell.execute_reply":"2024-06-05T17:52:20.271559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6c. L1-based regularization (Lasso)\nThe selected features are determined by their non-zero coefficients after applying L1 regularization (Lasso), prioritizing features that contribute to the model's predictive power while encouraging sparsity by shrinking some coefficients towards zero.","metadata":{}},{"cell_type":"code","source":"def load_data():\n    # Load your dataset\n    df\n    return df\n\ndef select_features_lasso(X, y, threshold='median'):\n    model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n    sfm = SelectFromModel(estimator=model, threshold=threshold)\n    sfm.fit(X, y)\n    selected_indices = sfm.get_support(indices=True)\n    selected_features = X.columns[selected_indices]\n    plot_lasso_coefficients(X, sfm, selected_indices)\n    return selected_features\n\ndef plot_lasso_coefficients(X, sfm, selected_indices):\n    coefficients_all = sfm.estimator_.coef_.flatten()\n    coefficients_selected = coefficients_all[selected_indices]\n    plt.figure(figsize=(15, 6))\n    plt.plot(np.abs(coefficients_all), marker='o', linestyle='None', markersize=5, color='green', label='All Features')\n    plt.plot(selected_indices, np.abs(coefficients_selected), marker='o', linestyle='None', markersize=5, color='blue', label='Selected Features')\n    plt.xticks(range(len(X.columns)), X.columns, rotation=90)\n    plt.xlabel('Features')\n    plt.ylabel('Absolute coefficient')\n    plt.title('Absolute coefficients of all features from L1 regularization (Lasso)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef train_and_evaluate_model(X, y, selected_features):\n    X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=42)\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\ndef main():\n    df = load_data()\n    X = df.drop('Target', axis=1)\n    y = df['Target']\n    \n    global selected_features_lasso\n    selected_features_lasso = select_features_lasso(X, y)\n    train_and_evaluate_model(X, y, selected_features_lasso)\n\nif __name__ == \"__main__\":\n    main()\n\n# Accessing selected_features_lasso outside the main function\nprint(\"Selected features using L1 regularization:\", selected_features_lasso)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:20.277411Z","iopub.execute_input":"2024-06-05T17:52:20.278091Z","iopub.status.idle":"2024-06-05T17:52:21.252513Z","shell.execute_reply.started":"2024-06-05T17:52:20.278018Z","shell.execute_reply":"2024-06-05T17:52:21.250887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_selected_features = set(selected_features_lasso) | set(selected_features_rfe) | set(selected_features_anova)\n\n# Convert the combined selected features to a list\ncombined_selected_features = list(combined_selected_features)\n\n# Display the combined selected features\nprint(combined_selected_features)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:21.259648Z","iopub.execute_input":"2024-06-05T17:52:21.260439Z","iopub.status.idle":"2024-06-05T17:52:21.277574Z","shell.execute_reply.started":"2024-06-05T17:52:21.260376Z","shell.execute_reply":"2024-06-05T17:52:21.275909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We combined the selected features from different feature selection techniques, including Lasso, RFE, and ANOVA, to leverage the strengths of each method and create a comprehensive feature set that captures the most relevant information for our model. By merging these sets, we aim to improve the model's predictive performance and robustness by incorporating diverse perspectives on feature importance and relevance.","metadata":{"execution":{"iopub.status.busy":"2024-04-11T13:27:20.821218Z","iopub.execute_input":"2024-04-11T13:27:20.821898Z","iopub.status.idle":"2024-04-11T13:27:20.829156Z","shell.execute_reply.started":"2024-04-11T13:27:20.821859Z","shell.execute_reply":"2024-04-11T13:27:20.827988Z"}}},{"cell_type":"markdown","source":"## 6d. Correlation-based feature selection\nIt helps identify and remove highly correlated features, reducing redundancy and multicollinearity in the feature set.","metadata":{}},{"cell_type":"code","source":"selected_features_with_target = combined_selected_features + [\"Target\"]\n\n# Select only the combined selected features and the target variable from the DataFrame\ndf = df[selected_features_with_target]\n\ncorr_matrix = df.drop(columns=['Target']).corr()\n\n# Mask for the upper triangle\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n# Plot the heatmap\nplt.figure(figsize=(22, 10))\nsns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:21.279946Z","iopub.execute_input":"2024-06-05T17:52:21.280878Z","iopub.status.idle":"2024-06-05T17:52:23.560506Z","shell.execute_reply.started":"2024-06-05T17:52:21.280787Z","shell.execute_reply":"2024-06-05T17:52:23.559328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = df.corr()\n\n# Create a mask for highly correlated features\n# The mask will be True for correlated pairs, and we'll keep the first feature\ncorrelation_mask = (correlation_matrix.abs() > 0.5) & (correlation_matrix != 1)\n\n# Identify columns (features) to drop\ncolumns_to_drop = set()\nfor feature in correlation_mask.columns:\n    correlated_features = correlation_mask.index[correlation_mask[feature]]\n    if len(correlated_features) > 0:\n        columns_to_drop.add(feature)\n\n# Remove highly correlated columns from the DataFrame\ndf = df.drop(columns=columns_to_drop)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:23.561984Z","iopub.execute_input":"2024-06-05T17:52:23.562374Z","iopub.status.idle":"2024-06-05T17:52:23.590808Z","shell.execute_reply.started":"2024-06-05T17:52:23.562343Z","shell.execute_reply":"2024-06-05T17:52:23.589471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Threshold Selection for Filtering Highly Correlated Features\n\nIn the analysis, I decided to set a threshold of 0.5 for filtering highly correlated features. This decision was based on several considerations:\n\n1. **Balance between Correlation Strength and Feature Retention**: A threshold of 0.5 strikes a balance between capturing moderately strong correlations and retaining a sufficient number of features for analysis. Features with correlation coefficients above 0.5 are considered moderately correlated and may contain redundant information, which could potentially lead to multicollinearity issues in modeling.\n\n2. **Alignment with Best Practices**: While there is no universally accepted threshold for filtering highly correlated features, a threshold of 0.5 aligns with common practices in the literature and is often used as a starting point for feature selection and dimensionality reduction tasks.\n\n3. **Domain Knowledge**: Domain-specific knowledge plays a crucial role in determining the appropriate threshold. In this analysis, a threshold of 0.5 was deemed suitable based on an understanding of the problem domain and the relationships between the variables.\n\n4. **Model Performance Consideration**: The selected threshold aims to strike a balance between reducing multicollinearity and preserving important information for modeling. It allows for the removal of highly correlated features that may not contribute significantly to predictive performance while retaining features with distinct information.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **7. Model Training and Evaluation**\n\n### Key tasks:\n* Model Training\n* Model Evaluation\n* Reciever operating curve","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:20:48.840722Z","iopub.execute_input":"2024-04-12T15:20:48.842871Z","iopub.status.idle":"2024-04-12T15:20:48.849056Z","shell.execute_reply.started":"2024-04-12T15:20:48.842802Z","shell.execute_reply":"2024-04-12T15:20:48.847522Z"}}},{"cell_type":"code","source":"# Step 1: Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Target']), df['Target'], test_size=0.2, random_state=42)\n\n# Step 2: Define models\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"Support Vector Machine\": SVC(random_state=42),\n    \"Logistic Regression\": LogisticRegression(random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n}\n\n# Step 3: Train each model\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    print(f\"{name} model trained\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:23.592246Z","iopub.execute_input":"2024-06-05T17:52:23.592577Z","iopub.status.idle":"2024-06-05T17:52:25.533175Z","shell.execute_reply.started":"2024-06-05T17:52:23.592550Z","shell.execute_reply":"2024-06-05T17:52:25.531969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Evaluate each model\nmetrics = {\n    \"Model\": [],\n    \"Accuracy\": [],\n    \"Precision\": [],\n    \"Recall\": [],\n    \"F1 Score\": []\n}\n\nbest_model = None\nbest_accuracy = 0\n\nfor name, model in models.items():\n    y_pred = model.predict(X_test)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    # Store metrics\n    metrics[\"Model\"].append(name)\n    metrics[\"Accuracy\"].append(accuracy)\n    metrics[\"Precision\"].append(precision)\n    metrics[\"Recall\"].append(recall)\n    metrics[\"F1 Score\"].append(f1)\n    \n    if accuracy > best_accuracy:\n        best_model = model\n        best_accuracy = accuracy\n\n# Step 5: Present evaluation metrics in a table\ndf_metrics = pd.DataFrame(metrics)\nprint(\"\\nEvaluation Metrics:\")\nprint(df_metrics)\n\nprint(f\"\\nThe best model is {best_model.__class__.__name__} with test accuracy: {best_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:25.534709Z","iopub.execute_input":"2024-06-05T17:52:25.535086Z","iopub.status.idle":"2024-06-05T17:52:25.720612Z","shell.execute_reply.started":"2024-06-05T17:52:25.535031Z","shell.execute_reply":"2024-06-05T17:52:25.719012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 7: Plot ROC curve for each model\nplt.figure(figsize=(12, 6))\n\n# Random Forest\nrf_fpr, rf_tpr, _ = roc_curve(y_test, models[\"Random Forest\"].predict_proba(X_test)[:, 1])\nplt.plot(rf_fpr, rf_tpr, label=f\"Random Forest (AUC = {roc_auc_score(y_test, models['Random Forest'].predict_proba(X_test)[:, 1]):.2f})\")\n\n# Support Vector Machine\nsvm_fpr, svm_tpr, _ = roc_curve(y_test, models[\"Support Vector Machine\"].decision_function(X_test))\nplt.plot(svm_fpr, svm_tpr, label=f\"Support Vector Machine (AUC = {roc_auc_score(y_test, models['Support Vector Machine'].decision_function(X_test)):.2f})\")\n\n# Logistic Regression\nlr_fpr, lr_tpr, _ = roc_curve(y_test, models[\"Logistic Regression\"].predict_proba(X_test)[:, 1])\nplt.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC = {roc_auc_score(y_test, models['Logistic Regression'].predict_proba(X_test)[:, 1]):.2f})\")\n\n# Gradient Boosting\ngb_fpr, gb_tpr, _ = roc_curve(y_test, models[\"Gradient Boosting\"].predict_proba(X_test)[:, 1])\nplt.plot(gb_fpr, gb_tpr, label=f\"Gradient Boosting (AUC = {roc_auc_score(y_test, models['Gradient Boosting'].predict_proba(X_test)[:, 1]):.2f})\")\n\n# Plot ROC curve for random guessing (no skill)\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T17:52:25.728013Z","iopub.execute_input":"2024-06-05T17:52:25.728672Z","iopub.status.idle":"2024-06-05T17:52:26.402452Z","shell.execute_reply.started":"2024-06-05T17:52:25.728616Z","shell.execute_reply":"2024-06-05T17:52:26.401134Z"},"trusted":true},"execution_count":null,"outputs":[]}]}